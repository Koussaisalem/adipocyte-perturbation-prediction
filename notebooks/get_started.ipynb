{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bcfb0b",
   "metadata": {},
   "source": [
    "# Adipocyte Perturbation: Quickstart\n",
    "End-to-end setup from repo clone to training and submission generation. Run cells top-to-bottom on a GPU-enabled machine with sufficient disk (≥100 GB recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bef1a",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "- Python 3.10+, CUDA-capable GPU, and disk headroom (≥100 GB).\n",
    "- Raw challenge files placed under `data/raw`:\n",
    "  - obesity_challenge_1.h5ad\n",
    "  - signature_genes.csv\n",
    "  - program_proportion.csv\n",
    "  - program_proportion_local_gtruth.csv\n",
    "  - predict_perturbations.txt\n",
    "  - gene_to_predict.txt\n",
    "- Git access to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938a125",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3199916407.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgit clone https://github.com/Koussaisalem/adipocyte-perturbation-prediction.git\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 1) Clone repo (skip if already inside)\n",
    "!git clone https://github.com/Koussaisalem/adipocyte-perturbation-prediction.git\n",
    "%cd adipocyte-perturbation-prediction\n",
    "\n",
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install -e \".[dev,notebooks]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Install core requirements (recommended for clean envs)\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f430cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Verify GPU availability\n",
    "!nvidia-smi || echo 'nvidia-smi not available'\n",
    "python - <<'PY'\n",
    "import torch\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('CUDA devices:', torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print('Device 0:', torch.cuda.get_device_name(0))\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077601ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Verify raw data presence\n",
    "!ls -lh data/raw/Challenge\n",
    "\n",
    "# ln -s /path/to/challenge/data data/raw/Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b) Unzip the small h5ad if needed\n",
    "!unzip -o data/raw/Challenge/obesity_challenge_1.h5ad.small.zip -d data/raw/Challenge\n",
    "!ls -lh data/raw/Challenge | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b5a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Run setup helper (checks files, creates all_genes.txt, directories)\n",
    "!bash setup_codespace.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e108f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Build Knowledge Graph (CollecTRI/DoRothEA + STRING)\n",
    "!python scripts/build_kg.py \\\n",
    "  --gene-list data/processed/all_genes.txt \\\n",
    "  --output data/kg/knowledge_graph.gpickle \\\n",
    "  --dorothea-levels A B \\\n",
    "  --string-threshold 700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c89535",
   "metadata": {},
   "source": [
    "## 7b. Fix Geneformer Assets (Git-LFS Workaround)\n",
    "On CamberCloud and similar environments without `apt-get` access, Git-LFS pointers in the Geneformer package need to be replaced with real `.pkl` files downloaded directly from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79089ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Geneformer .pkl files (Git-LFS pointers → real data)\n",
    "# This is needed on CamberCloud where apt-get install git-lfs is not available\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find geneformer package location\n",
    "gf_result = subprocess.run(\n",
    "    [sys.executable, \"-c\", \"import geneformer; print(geneformer.__path__[0])\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "gf_path = Path(gf_result.stdout.strip())\n",
    "print(f\"Geneformer package location: {gf_path}\")\n",
    "\n",
    "# List of .pkl files that need to be real (not Git-LFS pointers)\n",
    "pkl_files = [\n",
    "    \"gene_dictionaries_gc104M/gene_name_id_dict_gc104M.pkl\",\n",
    "    \"gene_dictionaries_gc104M/gene_median_dict_gc104M.pkl\",\n",
    "    \"gene_dictionaries_gc104M/ensembl_mapping_dict_gc104M.pkl\",\n",
    "    \"gene_dictionaries_gc104M/token_dict_gc104M.pkl\",\n",
    "]\n",
    "\n",
    "# Check if files are Git-LFS pointers\n",
    "def is_lfs_pointer(filepath):\n",
    "    if not filepath.exists():\n",
    "        return True\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        first_bytes = f.read(20)\n",
    "    return first_bytes.startswith(b\"version https://git-lfs\")\n",
    "\n",
    "needs_fix = any(is_lfs_pointer(gf_path / p) for p in pkl_files)\n",
    "\n",
    "if needs_fix:\n",
    "    print(\"Detected Git-LFS pointers. Downloading real .pkl files from HuggingFace...\")\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    \n",
    "    for rel_path in pkl_files:\n",
    "        local_path = gf_path / rel_path\n",
    "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"  Downloading {rel_path}...\")\n",
    "        downloaded = hf_hub_download(\n",
    "            repo_id=\"ctheodoris/Geneformer\",\n",
    "            filename=rel_path,\n",
    "            local_dir=\"/tmp/geneformer_assets\",\n",
    "            force_download=True\n",
    "        )\n",
    "        # Copy to geneformer package\n",
    "        import shutil\n",
    "        shutil.copy2(downloaded, local_path)\n",
    "        \n",
    "        # Verify\n",
    "        with open(local_path, \"rb\") as f:\n",
    "            first_bytes = f.read(10)\n",
    "        print(f\"    Verified: first bytes = {first_bytes[:10]}\")\n",
    "    \n",
    "    print(\"✓ All .pkl files fixed!\")\n",
    "else:\n",
    "    print(\"✓ Geneformer .pkl files are already valid (not LFS pointers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84938462",
   "metadata": {},
   "source": [
    "## 7c. Download Full Geneformer Model\n",
    "Downloads the complete Geneformer model including `pytorch_model.bin` (not just .pkl files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9faead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download full Geneformer model (including pytorch_model.bin)\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path(\"models/geneformer_full\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if model already downloaded\n",
    "model_subdir = model_dir / \"gf-12L-104M-i4096\"\n",
    "if (model_subdir / \"pytorch_model.bin\").exists():\n",
    "    print(f\"✓ Model already exists at {model_subdir}\")\n",
    "else:\n",
    "    print(\"Downloading full Geneformer model from HuggingFace...\")\n",
    "    print(\"This may take a few minutes (~500MB)...\")\n",
    "    \n",
    "    snapshot_download(\n",
    "        repo_id=\"ctheodoris/Geneformer\",\n",
    "        local_dir=str(model_dir),\n",
    "        # No allow_patterns filter - download everything including pytorch_model.bin\n",
    "    )\n",
    "    \n",
    "    # Verify\n",
    "    expected_model = model_dir / \"gf-12L-104M-i4096\" / \"pytorch_model.bin\"\n",
    "    if expected_model.exists():\n",
    "        print(f\"✓ Model downloaded successfully: {expected_model}\")\n",
    "        print(f\"  Size: {expected_model.stat().st_size / 1e6:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"⚠ Expected model file not found. Checking directory...\")\n",
    "        !ls -la {model_dir}\n",
    "        !ls -la {model_dir}/gf-12L-104M-i4096/ 2>/dev/null || echo \"Model subdirectory not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea07cc",
   "metadata": {},
   "source": [
    "## 7d. Prepare h5ad with Required Columns\n",
    "Geneformer requires `ensembl_id` in `var` and `n_counts` in `obs`. This cell adds these columns to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5db683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare h5ad with ensembl_id and n_counts columns for Geneformer\n",
    "import scanpy as sc\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Input and output paths\n",
    "input_h5ad = Path(\"data/raw/Challenge/obesity_challenge_1.h5ad\")\n",
    "output_h5ad = Path(\"data/raw/Challenge/obesity_challenge_1.prepared.h5ad\")\n",
    "\n",
    "if output_h5ad.exists():\n",
    "    print(f\"✓ Prepared file already exists: {output_h5ad}\")\n",
    "else:\n",
    "    print(f\"Loading {input_h5ad}...\")\n",
    "    adata = sc.read_h5ad(input_h5ad)\n",
    "    print(f\"  Shape: {adata.shape[0]} cells x {adata.shape[1]} genes\")\n",
    "    \n",
    "    # 1. Add ensembl_id column using Geneformer's gene name mapping\n",
    "    if \"ensembl_id\" not in adata.var.columns:\n",
    "        print(\"Adding ensembl_id column...\")\n",
    "        \n",
    "        # Find geneformer package and load its gene name dict\n",
    "        gf_result = subprocess.run(\n",
    "            [sys.executable, \"-c\", \"import geneformer; print(geneformer.__path__[0])\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        gf_path = Path(gf_result.stdout.strip())\n",
    "        gene_name_dict_path = gf_path / \"gene_dictionaries_gc104M\" / \"gene_name_id_dict_gc104M.pkl\"\n",
    "        \n",
    "        with open(gene_name_dict_path, \"rb\") as f:\n",
    "            gene_name_to_ensembl = pickle.load(f)\n",
    "        \n",
    "        # Map gene symbols to Ensembl IDs\n",
    "        ensembl_ids = []\n",
    "        missing_count = 0\n",
    "        for gene in adata.var_names:\n",
    "            if gene in gene_name_to_ensembl:\n",
    "                ensembl_ids.append(gene_name_to_ensembl[gene])\n",
    "            else:\n",
    "                ensembl_ids.append(f\"UNKNOWN_{gene}\")\n",
    "                missing_count += 1\n",
    "        \n",
    "        adata.var[\"ensembl_id\"] = ensembl_ids\n",
    "        mapped_count = len(adata.var) - missing_count\n",
    "        print(f\"  Mapped {mapped_count}/{len(adata.var)} genes ({100*mapped_count/len(adata.var):.1f}%)\")\n",
    "    else:\n",
    "        print(\"✓ ensembl_id column already exists\")\n",
    "    \n",
    "    # 2. Add n_counts column (total counts per cell)\n",
    "    if \"n_counts\" not in adata.obs.columns:\n",
    "        print(\"Adding n_counts column...\")\n",
    "        \n",
    "        # Handle both sparse and dense matrices\n",
    "        X = adata.X\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            n_counts = np.array(X.sum(axis=1)).flatten()\n",
    "        else:\n",
    "            n_counts = np.array(X.sum(axis=1)).flatten()\n",
    "        \n",
    "        adata.obs[\"n_counts\"] = n_counts\n",
    "        print(f\"  n_counts range: {n_counts.min():.0f} - {n_counts.max():.0f}\")\n",
    "    else:\n",
    "        print(\"✓ n_counts column already exists\")\n",
    "    \n",
    "    # Save prepared file\n",
    "    print(f\"Saving prepared file to {output_h5ad}...\")\n",
    "    adata.write_h5ad(output_h5ad)\n",
    "    print(f\"✓ Prepared file saved: {output_h5ad}\")\n",
    "    print(f\"  Size: {output_h5ad.stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c95a2d",
   "metadata": {},
   "source": [
    "## 7e. Create Fixed Embedding Extraction Script\n",
    "The original script has issues with tempfile directories. This creates a fixed version that uses stable paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/extract_embeddings_fixed.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fixed Geneformer embedding extraction script.\n",
    "Key fixes:\n",
    "  - Uses fixed work directory instead of tempfile (avoids tokenizer path issues)\n",
    "  - Uses input_identifier=chunk_path.stem (no .h5ad extension)\n",
    "  - Conservative memory settings for GPU environments\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent))\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_geneformer():\n",
    "    try:\n",
    "        from geneformer import TranscriptomeTokenizer, EmbExtractor\n",
    "    except ImportError:\n",
    "        logger.error(\"Geneformer not installed. Install with: pip install git+https://huggingface.co/ctheodoris/Geneformer.git\")\n",
    "        sys.exit(1)\n",
    "    return TranscriptomeTokenizer, EmbExtractor\n",
    "\n",
    "\n",
    "def clean_dir(path: Path):\n",
    "    if path.exists():\n",
    "        for item in path.iterdir():\n",
    "            if item.is_dir():\n",
    "                shutil.rmtree(item)\n",
    "            else:\n",
    "                item.unlink()\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Extract Geneformer gene embeddings with chunking (fixed version)\")\n",
    "    parser.add_argument(\"--h5ad-file\", required=True, help=\"Path to h5ad file (must have ensembl_id and n_counts)\")\n",
    "    parser.add_argument(\"--model-dir\", required=True, help=\"Path to Geneformer model directory (e.g., models/geneformer_full/gf-12L-104M-i4096)\")\n",
    "    parser.add_argument(\"--output\", default=\"data/processed/gene_embeddings.pt\", help=\"Path to save embeddings\")\n",
    "    parser.add_argument(\"--work-dir\", default=\"work/geneformer\", help=\"Working directory for intermediate files\")\n",
    "    parser.add_argument(\"--gene-list\", default=None, help=\"Optional file with genes to keep in output\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=1, help=\"Forward batch size (use 1 for low memory)\")\n",
    "    parser.add_argument(\"--max-cells\", type=int, default=1000, help=\"Total cells to process\")\n",
    "    parser.add_argument(\"--chunk-cells\", type=int, default=100, help=\"Cells per chunk for tokenization\")\n",
    "    parser.add_argument(\"--emb-layer\", type=int, choices=[-1, 0], default=-1, help=\"Embedding layer: -1 (2nd last) or 0 (last)\")\n",
    "    parser.add_argument(\"--random\", action=\"store_true\", help=\"Use random embeddings (testing only)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    output_path = Path(args.output)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Random mode for quick testing\n",
    "    if args.random:\n",
    "        import scanpy as sc\n",
    "        adata = sc.read_h5ad(args.h5ad_file, backed=\"r\")\n",
    "        genes = list(adata.var_names)\n",
    "        embedding_dim = 512\n",
    "        embeddings = {g: torch.randn(embedding_dim) for g in genes}\n",
    "        torch.save(embeddings, output_path)\n",
    "        logger.info(f\"Saved {len(embeddings)} random embeddings to {output_path}\")\n",
    "        return\n",
    "\n",
    "    TranscriptomeTokenizer, EmbExtractor = load_geneformer()\n",
    "    model_dir = args.model_dir\n",
    "\n",
    "    # Verify model exists\n",
    "    model_path = Path(model_dir)\n",
    "    if not (model_path / \"pytorch_model.bin\").exists() and not (model_path / \"model.safetensors\").exists():\n",
    "        logger.error(f\"Model not found at {model_dir}. Expected pytorch_model.bin or model.safetensors\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    import scanpy as sc\n",
    "\n",
    "    # Set up fixed work directory\n",
    "    work_path = Path(args.work_dir)\n",
    "    data_dir = work_path / \"data\"\n",
    "    token_dir = work_path / \"tokenized\"\n",
    "    emb_dir = work_path / \"embeddings\"\n",
    "    \n",
    "    for d in [data_dir, token_dir, emb_dir]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        clean_dir(d)\n",
    "\n",
    "    # Open backed to avoid loading whole file\n",
    "    adata = sc.read_h5ad(args.h5ad_file, backed=\"r\")\n",
    "    n_cells = adata.n_obs\n",
    "    logger.info(f\"Backed dataset: {n_cells} cells x {adata.n_vars} genes\")\n",
    "\n",
    "    # Determine total cells to process\n",
    "    total_cells = min(args.max_cells, n_cells)\n",
    "    chunk_size = min(args.chunk_cells, total_cells)\n",
    "    if chunk_size <= 0:\n",
    "        logger.error(\"Chunk size must be positive\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    agg_sum: dict[str, torch.Tensor] = {}\n",
    "    counts: dict[str, int] = {}\n",
    "    processed = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    tokenizer = TranscriptomeTokenizer(nproc=1, model_input_size=4096, special_token=True, model_version=\"V2\")\n",
    "\n",
    "    while processed < total_cells:\n",
    "        start = processed\n",
    "        end = min(processed + chunk_size, total_cells)\n",
    "        logger.info(f\"Processing chunk {chunk_idx}: cells {start} to {end} (total {total_cells})\")\n",
    "\n",
    "        # Load slice into memory\n",
    "        slice_indices = list(range(start, end))\n",
    "        chunk_adata = adata[slice_indices, :].to_memory()\n",
    "        \n",
    "        # Use a simple name without .h5ad extension for input_identifier\n",
    "        chunk_name = f\"chunk_{chunk_idx}\"\n",
    "        chunk_path = data_dir / f\"{chunk_name}.h5ad\"\n",
    "        chunk_adata.write_h5ad(chunk_path)\n",
    "        del chunk_adata\n",
    "\n",
    "        # Tokenize chunk - KEY FIX: use stem (no .h5ad extension)\n",
    "        clean_dir(token_dir)\n",
    "        logger.info(f\"Tokenizing with input_identifier={chunk_name}\")\n",
    "        tokenizer.tokenize_data(\n",
    "            data_directory=str(data_dir),\n",
    "            output_directory=str(token_dir),\n",
    "            output_prefix=f\"tokenized_{chunk_idx}\",\n",
    "            file_format=\"h5ad\",\n",
    "            input_identifier=chunk_name,  # No .h5ad extension!\n",
    "        )\n",
    "\n",
    "        tokenized_files = list(token_dir.glob(f\"tokenized_{chunk_idx}*.dataset\"))\n",
    "        if not tokenized_files:\n",
    "            logger.error(f\"No tokenized dataset produced for chunk {chunk_idx}\")\n",
    "            logger.info(f\"Token dir contents: {list(token_dir.iterdir())}\")\n",
    "            sys.exit(1)\n",
    "        tokenized_path = tokenized_files[0]\n",
    "        logger.info(f\"Tokenized dataset: {tokenized_path}\")\n",
    "\n",
    "        # Extract embeddings\n",
    "        clean_dir(emb_dir)\n",
    "        emb_extractor = EmbExtractor(\n",
    "            model_type=\"Pretrained\",\n",
    "            emb_mode=\"gene\",\n",
    "            gene_emb_style=\"mean_pool\",\n",
    "            max_ncells=None,\n",
    "            emb_layer=args.emb_layer,\n",
    "            forward_batch_size=args.batch_size,\n",
    "            nproc=1,\n",
    "            model_version=\"V2\",\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Extracting embeddings using model: {model_dir}\")\n",
    "        emb_extractor.extract_embs(\n",
    "            model_directory=model_dir,\n",
    "            input_data_file=str(tokenized_path),\n",
    "            output_directory=str(emb_dir),\n",
    "            output_prefix=f\"gene_embs_{chunk_idx}\",\n",
    "            output_torch_embs=True,\n",
    "        )\n",
    "\n",
    "        # Load embeddings from torch if available, else CSV\n",
    "        embeddings_chunk: dict[str, torch.Tensor] = {}\n",
    "        torch_files = list(emb_dir.glob(\"*.pt\"))\n",
    "        if torch_files:\n",
    "            data_pt = torch.load(torch_files[0], weights_only=False)\n",
    "            if isinstance(data_pt, dict):\n",
    "                embeddings_chunk = {k: (v if isinstance(v, torch.Tensor) else torch.tensor(v)) for k, v in data_pt.items()}\n",
    "        if not embeddings_chunk:\n",
    "            csv_files = list(emb_dir.glob(\"*.csv\"))\n",
    "            if csv_files:\n",
    "                emb_df = pd.read_csv(csv_files[0], index_col=0)\n",
    "                embeddings_chunk = {g: torch.tensor(emb_df.loc[g].values, dtype=torch.float32) for g in emb_df.index}\n",
    "        if not embeddings_chunk:\n",
    "            logger.error(\"Failed to load embeddings for chunk\")\n",
    "            logger.info(f\"Emb dir contents: {list(emb_dir.iterdir())}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        logger.info(f\"Loaded {len(embeddings_chunk)} gene embeddings from chunk {chunk_idx}\")\n",
    "\n",
    "        # Aggregate mean across chunks\n",
    "        for gene, vec in embeddings_chunk.items():\n",
    "            if gene not in agg_sum:\n",
    "                agg_sum[gene] = vec.clone().float()\n",
    "                counts[gene] = 1\n",
    "            else:\n",
    "                agg_sum[gene] += vec.float()\n",
    "                counts[gene] += 1\n",
    "\n",
    "        processed = end\n",
    "        chunk_idx += 1\n",
    "        \n",
    "        # Clean up chunk file to save space\n",
    "        if chunk_path.exists():\n",
    "            chunk_path.unlink()\n",
    "\n",
    "        # Clear CUDA cache to prevent OOM\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    adata.file.close()\n",
    "\n",
    "    # Finalize embeddings (mean across chunks)\n",
    "    final_embeddings = {g: agg_sum[g] / counts[g] for g in agg_sum}\n",
    "\n",
    "    # Filter to gene list if provided\n",
    "    if args.gene_list:\n",
    "        with open(args.gene_list) as f:\n",
    "            target = set(line.strip() for line in f if line.strip())\n",
    "        final_embeddings = {g: v for g, v in final_embeddings.items() if g in target}\n",
    "        logger.info(f\"Filtered to {len(final_embeddings)} genes from gene_list\")\n",
    "\n",
    "    torch.save(final_embeddings, output_path)\n",
    "    emb_dim = next(iter(final_embeddings.values())).shape[0] if final_embeddings else 0\n",
    "    logger.info(f\"Saved embeddings to {output_path}\")\n",
    "    logger.info(f\"  {len(final_embeddings)} genes x {emb_dim} dimensions\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47121cff",
   "metadata": {},
   "source": [
    "### Embedding Extraction Notes\n",
    "- Requires GPU and disk headroom; chunked to reduce memory.\n",
    "- Tune `--max-cells`, `--chunk-cells`, and `--batch-size` to your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f298d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Extract Geneformer embeddings using the FIXED script\n",
    "# Uses the prepared h5ad with ensembl_id and n_counts columns\n",
    "# Memory-safe settings: batch-size=1, chunk-cells=100-200\n",
    "\n",
    "# Adjust these based on your GPU memory:\n",
    "# - L4 (22GB): chunk-cells=100-200, max-cells=2000-5000\n",
    "# - A100 (40GB): chunk-cells=500, max-cells=10000+\n",
    "\n",
    "!python scripts/extract_embeddings_fixed.py \\\n",
    "  --h5ad-file data/raw/Challenge/obesity_challenge_1.prepared.h5ad \\\n",
    "  --model-dir models/geneformer_full/gf-12L-104M-i4096 \\\n",
    "  --max-cells 2000 \\\n",
    "  --chunk-cells 200 \\\n",
    "  --batch-size 1 \\\n",
    "  --output data/processed/gene_embeddings.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fd6c1",
   "metadata": {},
   "source": [
    "### Training Notes\n",
    "- Defaults: AdamW lr=1e-4, batch_size=64, epochs=100, precision=16-mixed, early stopping on val/mmd.\n",
    "- Adjust batch size or `accumulate_grad_batches` if you hit GPU OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b386d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Baseline training run\n",
    "!python scripts/train.py \\\n",
    "  --config configs/default.yaml \\\n",
    "  --seed 42 \\\n",
    "  2>&1 | tee experiments/logs/baseline_run.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d0d0b",
   "metadata": {},
   "source": [
    "### Experiment Variants (optional)\n",
    "- Increase MMD weight: create `configs/high_mmd.yaml` with `losses.mmd_weight: 0.2`, `losses.pearson_weight: 0.1`.\n",
    "- Deeper GAT: `gat_layers: 4`, `gat_heads: 16`, `gat_hidden_dim: 256` (lower batch if needed).\n",
    "- Higher PCA: `flow_matching.pca_components: 750` if memory allows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Generate submission from best checkpoint\n",
    "!python scripts/generate_submission.py \\\n",
    "  --checkpoint checkpoints/best.ckpt \\\n",
    "  --output-dir submissions \\\n",
    "  --n-cells 100 \\\n",
    "  --batch-size 10 \\\n",
    "  2>&1 | tee experiments/logs/inference.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b45385",
   "metadata": {},
   "source": [
    "### Quick Validation\n",
    "- Expected expression rows: 286,301 (including header).\n",
    "- NaN check on expression matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d45bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Validate submission files\n",
    "!wc -l submissions/expression_matrix.csv\n",
    "!head submissions/program_proportions.csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('submissions/expression_matrix.csv')\n",
    "print('NaNs:', df.isna().sum().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
